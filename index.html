<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



	<title>LonghuiWei's homepage</title>
	<style>

@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { padding : 0; margin : 0; font-size : 34px; }
h2 { font-size : 20px; margin : 0; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : #FFFFFF; }
/*body { padding : 0; font-family : Arial; font-size : 16px; background-color : #e0ffff; }*/
.title { width : 650px; margin : 20px auto; }
.container { width : 800px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
#bio {
    padding-top : 40px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 30px; margin-left : 30px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : rgb(40, 40, 119); }
.publication .links { position : relative; top : 15px }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
	</style>
	<!-- <script async="" src="./files/analytics.js"></script> -->
</head>

<body>
	<div class="title">
		<div id="sidebar"><img src="Me.jpeg" id="me" itemprop="photo" vspace="20 px" width="200 px"></div>
		<div id="bio">
			<h1>
				<span itemprop="name">Longhui Wei</span>
			</h1>
			<p style="line-height:25px;">
				Senior Researcher<br>
                Huawei Cloud <br>
                <!-- <br> -->
                
                Email: weilh2568@gmail.com
                <!-- <br> -->
               
			</p>
			<p class="external">
            <a href="https://www.linkedin.com/in/longhui-wei-42a32314b/" class="first">[CV]</a>
            <a href="https://github.com/JoinWei-PKU" class="first">[Github]</a>
            <a href="https://scholar.google.com/citations?user=thhnAhIAAAAJ&hl=zh-CN" class="first">[Google Scholar]</a> 
			</p>
		</div>
	</div>

	<div class="container">
		<!-- <h2>Short Bio</h2> -->
		<p> 
            <b>Brief Bio:</b> I am a senior researcher at Huawei Cloud, and lead the multimodal frontier research and applications. Moreover, I also focus on visual pre-training.  Before that, I received Doctor degress from <b>USTC</b> in 2023(Advised by Prof. Wengang Zhou and Prof. Qi Tian), received M.S. degree from <b>PKU</b> (Advised by Prof. Shiliang Zhang and Prof. Qi Tian), Beijing, China, in 2019. Currently, I'm interested in Multimodal&Visual Pre-training, Diffusion Model, Open-vocabulary Learning and some related fields.
		</p>
		<p>
			<font color="red"><b>I am looking for highly self-motivated interns or full-time employees. Please drop me an email with your CV, if you are insterested.</b></font>
		</p>

	</div>
	<div class="container">
	<h2>News</h2>
	<ul>
	<li> I have successfully defended my Doctor Thesis! Congratulations to myself!</li>
	<li>2023.09, One paper about Multimodal Understanding for ReID has been accepted by TMM.</li>
	<li>2023.07, One paper about Multimodal Generation work has been accepted by ACM MM'23.</li>
	<li>2023.07, One paper about Multimodal Prompt Learning work has been accepted by ICCV'23.</li>
	<li>2023.04, Two papers about Multimodal Continual Learning & Diffusion Model work has been accepted by ICML'23.</li>
	<li>2023.03, One paper about Self-supervised Learning work has been accepted by CVPR'23.</li>
	<li>2023.02, One paper about Self-supervised Learning work has been accepted by PR.</li>
	<li>2022.11, One paper about Text-to-Image Generation work has been accepted by AAAI'23.</li>
	<li>2022.09, One paper about Multimodal Pre-training work has been accepted by NeurIPS'22.</li>
	<li>2022.07, One paper about Multimodality has been accepted by ACM MM'22.</li>
	<li>2022.07, One paper about "MM-guided CV Pre-training"(called MVP) has been accepted by ECCV'22.</li>
	<li>2022.03, One paper about NAS has been accepted by CVPR'22.</li>
	<li>2022, One paper about NAS has been accepted by IJCV.</li>
	<li>2021.12, One paper about NAS has been accepted by T-IP.</li>
	<li>2021.12, One paper about Visual Pre-Training has been accepted by AAAI'22 as <b>ORAL paper</b>.</li>
	<li>2021.10, One paper about Few-shot Learning has been accepted by NIPS'21.</li>
	<li>One paper about Visual Pre-Training has been accepted by TMM.</li>
	<li>One paper about Transformer has been accepted by ICCV'21.</li>
	<li>One paper about NAS has been accepted by ACM Computing Surveys.</li>	
	<li>One paper about ReID has been accepted as <b>ORAL paper</b> by CVPR 2021.</li>
	<li>One paper about ReID has been accepted by T-CSVT.</li>	
	<li>One paper about NAS has been accepted by AAAI 2021.</li>
	<li>Two papers about ReID&AUTOML has been accepted by ECCV 2020, and one is accepted as <b>Spotlight paper</b>.</li>
	<li>We ranked first in Webvision Challenge 2020.</li>
	<li>One paper about NAS has been accepted by CVPR 2020.</li>
        <li>One paper about ReID has been accepted by AAAI 2020.</li>
	<li>I achieved Gaotong Scholarship.</li>
	<li>I achieved National Scholarship in Peking University.</li>
	<li>One paper about ReID has been accepted by TMM 2018. </li>
	<li>I achieved the Excellent Talents Scholarship of Cooperative Medianet Innovation Center.</li>
        <li> The <a href="https://github.com/pkuvmc/pkuvmc.github.io/tree/master/FG2018-Tutorial/">slides</a> of our tutorial on FG 2018: Person Re-Identification: Recent Advances and Challenges have been released ! </li>
        <li>One paper accepted as <b>Spotlight paper</b> by CVPR 2018. </li>
        <li>One paper about ReID has been accepted by ICMR 2018. </li>
        <li>One paper about ReID has been accepted by ACM MM 2017.</li>
    </ul>
	</div>

    <div class="container">
    	<h2>Services</h2>
        <ul>
    	<div>
    		<li><p>
    		Reviewer of ICCV, CVPR, ECCV, NeurIPS, ICML, AAAI
   			</p></li>
    	</div>
    	<div>
    		<li><p>
    		Reviewer of T-PAMI, IJCV, T-IP, T-MM, T-CSVT, Neurocomputing, IET computer vision, Multimedia System, etc.
   			</p></li>
    	</div>
    	<div>
    		<li><p>
            Speaker of FG2018 Tutorial, ChinaMM'22 workshop, VALSE'23 workshop, GAMESâ€˜23
    		</p></li>
    	</div>
    </ul>
    </div>
	

	<div class="container">
        <h2>Selected Publications</h2>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        Multi-granularity Matching Transformer for Text-based Person Search
                    </strong>
                    <br>
                    <br>
		     Liping Bao, <b>Longhui Wei</b>, Wengang Zhou, Lin Liu, Lingxi Xie, Houqiang Li, Qi Tian.
                    <br>
                    <em>IEEE Trans. on Multimedia, 2023</em>
                </p>
            </div>
	<br>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        Degeneration-Tuning: Using Scrambled Grid shield Unwanted Concepts from Stable Diffusion
                    </strong>
                    <br>
                    <br>
                     Zixuan Ni, <b>Longhui Wei</b>, Jiacheng Li, Siliang Tang, Yueting Zhuang and Qi Tian.
                    <br>
                    <em>ACM MM, 2023</em>
                </p>
            </div>
	<br>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models
                    </strong>
                    <br>
                    <br>
		     Juncheng Li*, Minghe Gao*, <b>Longhui Wei</b>, and et. al. (* denotes equal contribution)
                    <br>
                    <em>ICCV, 2023</em>
                </p>
            </div>
	<br>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        Continual Vision-Language Representaion Learning with Off-Diagonal Information
                    </strong>
                    <br>
                    <br>
                     Zixuan Ni, <b>Longhui Wei</b>, Siliang Tang, Yueting Zhuang and Qi Tian.
                    <br>
                    <em>ICML, 2023</em>
                </p>
            </div>
	<br>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        SDDM: Score-Decomposed Diffusion Models on Manifolds for Unpaired Image-to-Image Translation
                    </strong>
                    <br>
                    <br>
                     Shikun Sun, <b>Longhui Wei</b>, Junliang Xing, Jia Jia and Qi Tian.
                    <br>
                    <em>ICML, 2023</em>
                </p>
            </div>
	<br>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        Exploring the diversity and invariance in yourself for visual pre-training task
                    </strong>
                    <br>
                    <br>
                     <b>Longhui Wei</b>, Lingxi Xie, Wengang Zhou, Houqiang Li and Qi Tian.
                    <br>
                    <em>Pattern Recognition, 2023</em>
                </p>
            </div>
	<br>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        Fine-Grained Semantically Aligned Vision-Language Pre-Training
                    </strong>
                    <br>
                    <br>
                     Juncheng Li*, Xin He*, <b>Longhui Wei*</b> and et. al. (* denotes equal contribution)
                    <br>
                    <em>NeurIPS, 2022</em>
                </p>
            </div>
	<br>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        MVP: Multimodality-guided Visual Pre-Training
                    </strong>
                    <br>
                    <br>
                     <b>Longhui Wei</b> and et. al.
                    <br>
                    <em>ECCV, 2022</em>
                </p>
            </div>
	<br>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        Can Semantic Labels Assist Self-Supervised Visual Representation Learning?
                    </strong>
                    <br>
                    <br>
                     <b>Longhui Wei</b> and et. al.
                    <br>
                    <em>AAAI, 2022</em>
                </p>
            </div>
	<br>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        Rectifying the Shortcut Learning of Background: Shared Object Concentration for Few-Shot Image Recognition
                    </strong>
                    <br>
                    <br>
                     Luo Xu, <b>Longhui Wei</b> and et. al.
                    <br>
                    <em>NeurIPS, 2021</em>
                </p>
            </div>
	<br>
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        Visformer: The Vision-friendly Transformer
                    </strong>
                    <br>
                    <br>
                     Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, <b>Longhui Wei</b>, Qi Tian:.
                    <br>
                    <em>ICCV, 2021</em>
                </p>
            </div>
	<br>	
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        UnrealPerson: An Adaptive Pipeline towards Costless Person Re-identification
                    </strong>
                    <br>
                    <br>
                     Tianyu Zhang, Lingxi Xie, <b>Longhui Wei</b>, Zijie Zhuang, Yongfei Zhang, Bo Li and Qi Tian.
                    <br>
                    <em>CVPR, 2021, ORAL</em>
                </p>
            </div>
	<br>	
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        Camera-based Batch Normalization: An Effective Distribution Alignment Method for Person Re-identification
                    </strong>
                    <br>
                    <br>
                     Zijie Zhuang, <b>Longhui Wei</b>, Lingxi Xie, Haizhou Ai and Qi Tian.
                    <br>
                    <em>T-CSVT, 2021</em>
                </p>
            </div>
	<br>		
		
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        <a href="https://arxiv.org/pdf/2003.11342.pdf"> Circumventing Outliers of AutoAugment with Knowledge Distillation</a>
                    </strong>
                    <br>
                    <br>
                     <b>Longhui Wei</b>, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang and Qi Tian.
                    <br>
                    <em>ECCV, 2020, Spotlight</em>
                </p>
            </div>
	<br>		

		
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        <a href="https://openreview.net/forum?id=BJlgt2EYwr"> Disassembling the Dataset: A Camera Alignment Mechanism for Multiple Tasks in Person Re-identification</a>
                    </strong>
                    <br>
                    <br>
                     Zijie Zhuang, <b>Longhui Wei</b>, and et al.
                    <br>
                    <em>ECCV, 2020</em>
                </p>
            </div>
	<br>
		
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Network_Adjustment_Channel_Search_Guided_by_FLOPs_Utilization_Ratio_CVPR_2020_paper.pdf"> Network Adjustment: Channel Search Guided by FLOPs Utilization Ratio</a>
                    </strong>
                    <br>
                    <br>
                     Zhengsu Chen, Jianwei Niu, Lingxi Xie, Xuefeng Liu, <b>Longhui Wei</b> and Qi Tian.
                    <br>
                    <em>CVPR, 2020</em>
                </p>
            </div>
	<br>	
		
		
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        <a href="https://arxiv.org/abs/1909.10848"> Single Camera Training for Person Re-Identification</a>
                    </strong>
                    <br>
                    <br>
                     Tianyu Zhang, Lingxi Xie, <b>Longhui Wei</b>, Yongfei Zhang, Bo Li and Qi Tian.
                    <br>
                    <em>AAAI, 2020</em>
                </p>
            </div>
	<br>		
		
	<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8466034"> GLAD: Global-Local-Alignement Descriptor for Scalable Person Re-Identification</a>
                    </strong>
                    <br>
                    <br>
                     <b>Longhui Wei</b>, Shiliang Zhang, Wen Gao and Qi Tian.
                    <br>
                    <em>IEEE Trans. on Multimedia, 2018</em>
                </p>
            </div>
	<br>
		
		<div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        <a href="https://dl.acm.org/citation.cfm?id=3206086"> VP-ReID: Vehicle and Person Re-Identification System</a>
                    </strong>
                    <br>
                    <br>
                     <b>Longhui Wei</b>, Xiaobin Liu, Jianing Zhang and Shiliang Zhang
                    <br>
                    <em>In Proc. of the 2018 ACM on International Conference on Multimedia Retrieval(ICMR), 2018, demo paper</em>
                </p>
            </div>
		
	<br>
	
         <div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        <a href="https://arxiv.org/abs/1711.08565">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</a>
                    </strong>
                    <br>
                    <br>
                     <b>Longhui Wei</b>, Shiliang Zhang, Wen Gao and Qi Tian.
                    <br>
                    <em>In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, Spotlight</em>
                </p>
            </div>
	
         <br>
	
         <div class="publication">
                <!-- <img src="./img/iccv2017_logo.jpg" class="publogo" width="200 px"> -->
                <p> 
                    <strong>
                        <a href="https://arxiv.org/abs/1709.04329">GLAD: Global-Local-Alignement Descriptor for Pedestrain Retrieval</a>
                    </strong>
                    <br>
                    <br>
                    <b>Longhui Wei</b>, Shiliang Zhang, Hantao Yao, Wen Gao and Qi Tian.
                    <br>
                    <em>In Proc. ACM Conference on Multimedia (ACM MM), 2017, full paper</em>
                </p>
            </div>
	
        <br>

       
    </div>
    



</body></html>
